# -*- coding: utf-8 -*-
"""
TW50 TOP5 â€” å‹ç‡å¼·åŒ–ç‰ˆï¼ˆå®Œæ•´æª”ï¼‰
yfinance + TWSEå‚™æ´ã€è³‡æ–™æ¶ˆæ¯’ã€é˜²å‘†å¯«å…¥ã€Topæ¸…å–®èˆ‡é€²å‡ºå ´å»ºè­°ã€ATRé¢¨æ§

ç‰ˆæœ¬ï¼šv2025-09-04-winrate-pro

åˆ†é è¼¸å‡ºï¼š
  - TW50_finï¼ˆé‡‘èï¼‰
  - TW50_nonfinï¼ˆéé‡‘èï¼‰
  - Top10_nonfinï¼ˆéé‡‘ Top10ï¼šRSIã€Volume æ’åºï¼‰
  - Hot20_nonfinï¼ˆéé‡‘ æˆäº¤é‡å‰20ï¼‰
  - Top5_hot20ï¼ˆHot20è£¡å†æŒ‘Top5ï¼Œé™„é€²å‡ºå ´èˆ‡åœæ/åœåˆ©ï¼‰
  - äº¤æ¥æœ¬ï¼ˆç°¡è¦èªªæ˜èˆ‡ Roadmapï¼‰

GitHub Actions éœ€è¦çš„ Secretsï¼š
  - SHEET_ID
  - GCP_SERVICE_ACCOUNT_JSON
"""

import os, json, time
import numpy as np
import pandas as pd
import requests
import yfinance as yf

import gspread
from gspread_dataframe import set_with_dataframe

# ========== ä»£è™Ÿâ†”åç¨±ï¼ˆå¯ä»¥è‡ªå·±æ…¢æ…¢è£œï¼›ç¼ºçš„å°±ç•™ç©ºç™½ä¸å½±éŸ¿è¨ˆç®—ï¼‰ ==========
TICKER_NAME_MAP = {
    # éé‡‘èï¼ˆéƒ¨ä»½ç¤ºæ„ï¼‰
    "2330.TW":"å°ç©é›»","2317.TW":"é´»æµ·","2454.TW":"è¯ç™¼ç§‘","2303.TW":"è¯é›»","2308.TW":"å°é”é›»",
    "2382.TW":"å»£é”","2379.TW":"ç‘æ˜±","2395.TW":"ç ”è¯","2412.TW":"ä¸­è¯é›»","1216.TW":"çµ±ä¸€",
    "1301.TW":"å°å¡‘","1326.TW":"å°åŒ–","1402.TW":"é æ±æ–°","1101.TW":"å°æ³¥","1102.TW":"äºæ³¥",
    "2002.TW":"ä¸­é‹¼","3008.TW":"å¤§ç«‹å…‰","3711.TW":"æ—¥æœˆå…‰æŠ•æ§","2603.TW":"é•·æ¦®","2609.TW":"é™½æ˜",
    "2615.TW":"è¬æµ·","3481.TW":"ç¾¤å‰µ","3006.TW":"æ™¶è±ªç§‘","2408.TW":"å—äºç§‘",
    # é‡‘èï¼ˆ28xx + å¸¸è¦‹é‡‘èè‚¡ï¼‰
    "2880.TW":"è¯å—é‡‘","2881.TW":"å¯Œé‚¦é‡‘","2882.TW":"åœ‹æ³°é‡‘","2883.TW":"é–‹ç™¼é‡‘","2884.TW":"ç‰å±±é‡‘",
    "2885.TW":"å…ƒå¤§é‡‘","2886.TW":"å…†è±é‡‘","2887.TW":"å°æ–°é‡‘","2888.TW":"æ–°å…‰é‡‘","2889.TW":"åœ‹ç¥¨é‡‘",
    "2890.TW":"æ°¸è±é‡‘","2891.TW":"ä¸­ä¿¡é‡‘","2892.TW":"ç¬¬ä¸€é‡‘","2897.TW":"ç‹é“éŠ€è¡Œ","2898.TW":"å®‰æ³°éŠ€",
    "5871.TW":"ä¸­ç§Ÿ-KY","5876.TW":"ä¸Šæµ·å•†éŠ€",
}

FIN_TICKERS = {t for t in TICKER_NAME_MAP if t.startswith("28")}
FIN_TICKERS.update({"5871.TW", "5876.TW"})  # é‡‘èä½†ä¸æ˜¯ 28xx çš„

# ========== å°å·¥å…· ==========
def taipei_now_str():
    return pd.Timestamp.now(tz="Asia/Taipei").strftime("%Y-%m-%d %H:%M")

def get_gspread_client():
    js = os.environ.get("GCP_SERVICE_ACCOUNT_JSON", "")
    if not js:
        raise RuntimeError("ç¼ºå°‘ GCP_SERVICE_ACCOUNT_JSON Secret")
    try:
        data = json.loads(js)
    except Exception as e:
        raise RuntimeError("GCP_SERVICE_ACCOUNT_JSON ä¸æ˜¯åˆæ³• JSON") from e
    return gspread.service_account_from_dict(data)

def get_sheet():
    sid = os.environ.get("SHEET_ID", "")
    if not sid:
        raise RuntimeError("ç¼ºå°‘ SHEET_ID Secret")
    print("[INFO] Target SHEET_ID:", sid)
    return get_gspread_client().open_by_key(sid)

def get_or_create(sh, title, rows=2000, cols=30):
    for ws in sh.worksheets():
        if ws.title == title:
            return ws
    return sh.add_worksheet(title=title, rows=rows, cols=cols)

def sanitize_df(df: pd.DataFrame) -> pd.DataFrame:
    """é¿å… gspread å‚³è¼¸ Protobuf listValue éŒ¯èª¤ï¼šdatetimeâ†’strã€NaNâ†’Noneã€infâ†’NaNã€‚"""
    out = df.copy()
    # datetime è½‰å­—ä¸²
    for c in out.columns:
        if np.issubdtype(out[c].dtype, np.datetime64):
            out[c] = out[c].astype(str)
    # æ•¸å€¼ç‰¹ä¾‹è™•ç†
    out.replace([np.inf, -np.inf], np.nan, inplace=True)
    out = out.where(pd.notnull(out), None)
    # æ¬„åä¿è­‰æ˜¯å­—ä¸²
    out.columns = [str(c) for c in out.columns]
    # index ä¸ä¸Šå‚³
    return out

def upsert_df(ws, df, stamp_text):
    ws.clear()
    ws.update("A1", [[f"è³‡æ–™æˆªè‡³ (Asia/Taipei): {stamp_text}"]])
    if df is None or df.empty:
        ws.update("A3", [["No Data"]])
        return
    clean = sanitize_df(df)
    set_with_dataframe(ws, clean, row=2, include_index=False, include_column_header=True)

# ========== æŒ‡æ¨™è¨ˆç®— ==========
def add_indicators(df: pd.DataFrame) -> pd.DataFrame:
    df = df.sort_index().copy()
    # å‡ç·š
    df["SMA20"]  = df["Close"].rolling(20, min_periods=20).mean()
    df["SMA50"]  = df["Close"].rolling(50, min_periods=50).mean()
    df["SMA200"] = df["Close"].rolling(200, min_periods=200).mean()
    # æˆäº¤é‡ 20MA
    df["Vol20"] = df["Volume"].rolling(20, min_periods=20).mean()
    # RSI14ï¼ˆWilderï¼‰
    delta = df["Close"].diff()
    gain = delta.clip(lower=0).rolling(14, min_periods=14).mean()
    loss = (-delta.clip(upper=0)).rolling(14, min_periods=14).mean()
    rs = gain / loss.replace(0, np.nan)
    df["RSI14"] = 100 - (100 / (1 + rs))
    # å¸ƒæ—é€šé“
    mid = df["Close"].rolling(20, min_periods=20).mean()
    std = df["Close"].rolling(20, min_periods=20).std()
    df["BB_Mid"]   = mid
    df["BB_Upper"] = mid + 2 * std
    df["BB_Lower"] = mid - 2 * std
    # ATR14ï¼ˆé¢¨æ§ï¼‰
    prev_close = df["Close"].shift(1)
    tr1 = df["High"] - df["Low"]
    tr2 = (df["High"] - prev_close).abs()
    tr3 = (df["Low"] - prev_close).abs()
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    df["ATR14"] = tr.rolling(14, min_periods=14).mean()
    return df

# ========== ä¸‹è¼‰è³‡æ–™ï¼šyfinance ä¸»ã€TWSE å‚™æ´ ==========
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

def fetch_yf_history(ticker: str, period="12mo", interval="1d") -> pd.DataFrame | None:
    try:
        raw = yf.download(ticker, period=period, interval=interval, auto_adjust=False, progress=False)
    except Exception:
        return None
    if raw is None or raw.empty:
        return None
    if isinstance(raw.columns, pd.MultiIndex):
        raw.columns = [c[0] for c in raw.columns]
    df = raw[["Open","High","Low","Close","Volume"]].copy()
    return df

def _twse_month_df(stock_no: str, yyyymmdd: str) -> pd.DataFrame:
    url = "https://www.twse.com.tw/exchangeReport/STOCK_DAY"
    params = {"response":"json","date":yyyymmdd,"stockNo":stock_no}
    r = requests.get(url, params=params, headers=HEADERS, timeout=12)
    r.raise_for_status()
    js = r.json()
    if js.get("stat") != "OK" or "data" not in js:
        return pd.DataFrame()
    cols = js["fields"]
    df = pd.DataFrame(js["data"], columns=cols)

    def _num(x):
        try:
            return float(str(x).replace(",","").replace("--",""))
        except:
            return np.nan

    df = df.rename(columns={
        "æ—¥æœŸ":"Date","é–‹ç›¤åƒ¹":"Open","æœ€é«˜åƒ¹":"High","æœ€ä½åƒ¹":"Low","æ”¶ç›¤åƒ¹":"Close","æˆäº¤è‚¡æ•¸":"Volume"
    })
    df["Date"] = pd.to_datetime(df["Date"].str.replace("/","-"), format="%Y-%m-%d", errors="coerce")
    for c in ["Open","High","Low","Close","Volume"]:
        df[c] = df[c].apply(_num)
    df = df[["Date","Open","High","Low","Close","Volume"]].dropna(subset=["Close"])
    return df.set_index("Date").sort_index()

def fetch_twse_history(ticker: str, months: int = 12) -> pd.DataFrame | None:
    stock_no = ticker.split(".")[0]
    today = pd.Timestamp.now(tz="Asia/Taipei")
    pieces = []
    for m in range(months):
        dt = today - pd.DateOffset(months=m)
        yyyymmdd = f"{dt.year}{dt.month:02d}01"
        try:
            part = _twse_month_df(stock_no, yyyymmdd)
            if not part.empty:
                pieces.append(part)
        except Exception:
            pass
        time.sleep(0.35)  # ç¦®è²Œæ€§é–“éš”ï¼Œé¿å…è¢«æ“‹
    if not pieces:
        return None
    df = pd.concat(pieces).sort_index()
    df = df[~df.index.duplicated(keep="last")]
    return df[["Open","High","Low","Close","Volume"]]

def fetch_history_with_fallback(ticker: str) -> pd.DataFrame | None:
    df = fetch_yf_history(ticker)
    if df is not None and not df.empty:
        return df
    print(f"[INFO] yfinance ç„¡è³‡æ–™ â†’ æ”¹ç”¨ TWSEï¼š{ticker}")
    return fetch_twse_history(ticker, months=12)

# ========== äº¤æ¥æœ¬ï¼ˆèªªæ˜åˆ†é ï¼‰ ==========
def update_roadmap(sh, stamp):
    ws = get_or_create(sh, "äº¤æ¥æœ¬", rows=300, cols=8)
    rows = []
    rows.append([f"äº¤æ¥æœ¬ï¼ˆè‡ªå‹•æ›´æ–°ï¼‰ï½œæœ€å¾Œæ›´æ–°ï¼š{stamp}"])
    rows.append([])
    rows.append(["å·²å®Œæˆ âœ…","èªªæ˜"])
    rows += [
        ["æ¯æ—¥è‡ªå‹•åŒ–","GitHub Actions â†’ TW50 â†’ Google Sheet"],
        ["æŠ€è¡“æŒ‡æ¨™","SMA20/50/200ã€RSI14ã€å¸ƒæ—å¸¶ã€Vol20ã€ATR14"],
        ["åˆ†é ","TW50_fin / TW50_nonfin / Top10_nonfin / Hot20_nonfin / Top5_hot20"],
        ["é˜²å‘†","yfinanceâ†’TWSE å‚™æ´ï¼›æŠ“ä¸åˆ°è‡ªå‹•è·³éï¼›å¯«å…¥å‰æ¶ˆæ¯’"],
    ]
    rows.append([])
    rows.append(["é€²è¡Œä¸­ ğŸ› ","èªªæ˜"])
    rows += [
        ["å‹ç‡æå‡","æˆäº¤é‡â‰¥Vol20 + è¶¨å‹¢åŒå‘(å¤šé ­/ç©ºé ­) + åš´è¬¹é–€æª»(å¸ƒæ—%b/RSI)"],
        ["åç¨±è£œé½Š","ä»£è™Ÿâ†”ä¸­æ–‡åç¨±ä¿åº•ä¸ç©ºç™½ï¼ˆç¼ºçš„æ—¥å¾Œè£œé½Šï¼‰"],
    ]
    rows.append([])
    rows.append(["æœªä¾† ğŸš€","èªªæ˜"])
    rows += [
        ["ç±Œç¢¼é¢","å¤–è³‡/æŠ•ä¿¡/è‡ªç‡Ÿå•†è²·è³£è¶…"],
        ["åŸºæœ¬é¢","EPSã€æ®–åˆ©ç‡"],
        ["é€šçŸ¥","LINE/Email æ¨æ’­ Top5 è¨Šè™Ÿ"],
        ["ç›¤ä¸­","éœ€åˆ¸å•†API/ä»˜è²»å³æ™‚æµ"],
        ["Dashboard","æŠ€è¡“ï¼‹ç±Œç¢¼ï¼‹åŸºæœ¬é¢ â†’ å¤šç©ºåˆ†æ•¸"],
    ]
    ws.clear()
    ws.update("A1", rows)

# ========== ä¸»æµç¨‹ ==========
def main():
    print("== TW50 TOP5ï¼ˆwinrate-proï¼‰==")
    sh = get_sheet()
    stamp = taipei_now_str()

    # è®€ config.jsonï¼ˆå¯æ”¾ tickers æ¸…å–®ï¼›æ²’æœ‰å°±ç”¨å…§å»º MAP çš„ keyï¼‰
    tickers = []
    if os.path.exists("config.json"):
        try:
            with open("config.json","r",encoding="utf-8") as f:
                cfg = json.load(f)
                tickers = cfg.get("tickers") or cfg.get("TW50") or []
        except Exception as e:
            print("[WARN] è®€å– config.json å¤±æ•—ï¼Œæ”¹ç”¨å…§å»ºæ¸…å–®", e)
    if not tickers:
        tickers = list(TICKER_NAME_MAP.keys())

    rows, failed = [], []
    for t in tickers:
        hist = fetch_history_with_fallback(t)
        if hist is None or hist.empty:
            print(f"[WARN] {t} æŸ¥ç„¡æ—¥ç·šè³‡æ–™ï¼Œå·²è·³é")
            failed.append(t)
            continue
        df = add_indicators(hist)
        last = df.tail(1).copy()  # å–æœ€æ–°ä¸€åˆ—
        # è£œå…¬å¸åç¨±ï¼ˆæ²’æœ‰å°±ç©ºå­—ä¸²ï¼Œä¸å½±éŸ¿ï¼‰
        cname = TICKER_NAME_MAP.get(t, "")
        last.insert(0, "å…¬å¸åç¨±", cname)
        last.insert(0, "è‚¡ç¥¨ä»£è™Ÿ", t)
        last = last.reset_index().rename(columns={"index":"Date"})
        rows.append(last)

    if not rows:
        raise RuntimeError("æœ¬æ¬¡æ²’æœ‰ä»»ä½•ä»£è™ŸæˆåŠŸæŠ“åˆ°è³‡æ–™")

    df_all = pd.concat(rows, ignore_index=True)

    # é‡‘è / éé‡‘è åˆ†æµ
    is_fin = df_all["è‚¡ç¥¨ä»£è™Ÿ"].isin(FIN_TICKERS) | df_all["è‚¡ç¥¨ä»£è™Ÿ"].str.startswith("28")
    df_fin    = df_all[is_fin].copy()
    df_nonfin = df_all[~is_fin].copy()

    # å…¨é‡æ¬„ä½
    base_cols = ["è‚¡ç¥¨ä»£è™Ÿ","å…¬å¸åç¨±","Date","Open","High","Low","Close","Volume","Vol20",
                 "RSI14","SMA20","SMA50","SMA200","BB_Lower","BB_Mid","BB_Upper","ATR14"]
    base_cols = [c for c in base_cols if c in df_all.columns]
    df_fin_all    = df_fin[base_cols].copy()
    df_nonfin_all = df_nonfin[base_cols].copy()

    # Top10ï¼ˆéé‡‘ï¼‰ï¼šRSIã€Volume é«˜åˆ°ä½
    top10 = df_nonfin.sort_values(["RSI14","Volume"], ascending=[False, False]).head(10).copy()

    # Hot20ï¼ˆéé‡‘ï¼‰ï¼šæˆäº¤é‡æœ€é«˜ 20
    hot20 = df_nonfin.sort_values("Volume", ascending=False).head(20).copy()

    # Top5ï¼šHot20 ä¸­å†ä¾ RSIã€Volume æŒ‘å‰5
    top5 = hot20.sort_values(["RSI14","Volume"], ascending=[False, False]).head(5).copy()

    # å¸ƒæ—%b
    bb_range = (top5["BB_Upper"] - top5["BB_Lower"]).replace(0, np.nan)
    top5["BB_percent"] = (top5["Close"] - top5["BB_Lower"]) / bb_range

    # è¶¨å‹¢éæ¿¾ï¼ˆåŒå‘æ‰æ‰“è¨Šè™Ÿï¼‰
    top5["å¤šé ­"] = (top5["Close"] > top5["SMA50"]) & (top5["SMA50"] > top5["SMA200"])
    top5["ç©ºé ­"] = (top5["Close"] < top5["SMA50"]) & (top5["SMA50"] < top5["SMA200"])

    # æˆäº¤é‡éæ¿¾ï¼ˆæ”¾é‡æ‰æœ‰æ•ˆï¼‰
    top5["VolOK"] = top5["Volume"] >= top5["Vol20"]

    # åš´è¬¹é–€æª»ï¼ˆæ›´ä¿å®ˆï¼‰
    def signal_pro(r):
        if r["VolOK"]:
            # å¤šé ­åªåšå¤šã€ç©ºé ­åªåšç©º
            if r["å¤šé ­"] and (
                (pd.notna(r["BB_percent"]) and r["BB_percent"] <= 0.12) or
                (pd.notna(r["RSI14"]) and r["RSI14"] <= 38)
            ):
                return "è²·é€²"
            if r["ç©ºé ­"] and (
                (pd.notna(r["BB_percent"]) and r["BB_percent"] >= 0.88) or
                (pd.notna(r["RSI14"]) and r["RSI14"] >= 62)
            ):
                return "è³£å‡º"
        return "è§€æœ›"

    top5["è¨Šè™Ÿ"] = top5.apply(signal_pro, axis=1)

    # å»ºè­°é€²å‡ºå ´ï¼ˆå¸ƒæ—å¸¶å€é–“ï¼‰
    top5["å»ºè­°é€²å ´ä¸‹ç•Œ"] = top5["BB_Lower"]
    top5["å»ºè­°é€²å ´ä¸Šç•Œ"] = top5["BB_Mid"]
    top5["å»ºè­°å‡ºå ´ä¸‹ç•Œ"] = top5["BB_Mid"]
    top5["å»ºè­°å‡ºå ´ä¸Šç•Œ"] = top5["BB_Upper"]

    # åƒè€ƒè·é›¢ï¼ˆç™¾åˆ†æ¯”ï¼‰
    top5["è·é›¢é€²å ´%"] = np.where(
        pd.notna(top5["BB_Mid"]) & (top5["Close"] <= top5["BB_Mid"]),
        (top5["Close"] - top5["BB_Lower"]) / top5["Close"] * 100, 0.0
    )
    top5["è·é›¢å‡ºå ´%"] = np.where(
        pd.notna(top5["BB_Mid"]) & (top5["Close"] >= top5["BB_Mid"]),
        (top5["BB_Upper"] - top5["Close"]) / top5["Close"] * 100, 0.0
    )

    # é¢¨æ§ï¼ˆATRï¼‰
    # åœæ = 1Ã—ATRï¼›åœåˆ© = 2.5Ã—ATRï¼ˆå¯èª¿ï¼‰
    top5["å»ºè­°åœæ%"] = (top5["ATR14"] / top5["Close"]) * 100
    top5["å»ºè­°åœåˆ©%"] = (top5["ATR14"] * 2.5 / top5["Close"]) * 100

    # Top5è¼¸å‡ºæ¬„ä½
    top5_cols = [
        "è‚¡ç¥¨ä»£è™Ÿ","å…¬å¸åç¨±","Date","Close","RSI14","BB_percent","å¤šé ­","ç©ºé ­","VolOK","è¨Šè™Ÿ",
        "å»ºè­°é€²å ´ä¸‹ç•Œ","å»ºè­°é€²å ´ä¸Šç•Œ","å»ºè­°å‡ºå ´ä¸‹ç•Œ","å»ºè­°å‡ºå ´ä¸Šç•Œ",
        "è·é›¢é€²å ´%","è·é›¢å‡ºå ´%","å»ºè­°åœæ%","å»ºè­°åœåˆ©%"
    ]
    top5_out = top5[top5_cols].copy()

    # ====== å¯«å…¥ Google Sheet ======
    upsert_df(get_or_create(sh,"TW50_fin"),     df_fin_all,    stamp)
    upsert_df(get_or_create(sh,"TW50_nonfin"),  df_nonfin_all, stamp)
    upsert_df(get_or_create(sh,"Top10_nonfin"), top10,         stamp)
    upsert_df(get_or_create(sh,"Hot20_nonfin"), hot20,         stamp)
    upsert_df(get_or_create(sh,"Top5_hot20"),   top5_out,      stamp)

    # äº¤æ¥æœ¬
    update_roadmap(sh, stamp)

    print(f"[INFO] Update å®Œæˆï¼šæˆåŠŸ {len(df_all)} æª”ï¼›è·³é {len(failed)} æª” -> {failed}")


if __name__ == "__main__":
    main()

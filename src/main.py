# -*- coding: utf-8 -*-
"""
TW50 TOP5 â€” yfinance + TWSE å‚™æ´ï¼ˆå¼·åŒ–å¯«å…¥é˜²å‘† + å‹ç‡ç‰ˆè¨Šè™Ÿ + äº¤æ¥æœ¬ï¼‰
ç‰ˆæœ¬ï¼šv2025-09-04-roadmap-volume

Secretsï¼ˆGitHub Actionsï¼‰ï¼š
  - SHEET_ID
  - GCP_SERVICE_ACCOUNT_JSON

è¼¸å‡ºåˆ†é ï¼š
  - TW50_fin / TW50_nonfin / Top10_nonfin / Hot20_nonfin / Top5_hot20 / äº¤æ¥æœ¬

Top5_hot20 æ¬„ä½ï¼š
  è‚¡ç¥¨ä»£è™Ÿã€å…¬å¸åç¨±ã€Dateã€æ”¶ç›¤åƒ¹ã€RSI14ã€å¸ƒæ—%bã€è¨Šè™Ÿï¼ˆè²·/è³£/è§€æœ›ï¼‰ã€
  å»ºè­°é€²å ´ä¸‹ç•Œ/ä¸Šç•Œã€å»ºè­°å‡ºå ´ä¸‹ç•Œ/ä¸Šç•Œã€è·é›¢é€²å ´%ã€è·é›¢å‡ºå ´%ã€Volumeã€Vol20ã€
  SMA20/50/200ã€BB_*
"""

import os, json, time
import numpy as np
import pandas as pd
import requests
import yfinance as yf
import gspread
from gspread_dataframe import set_with_dataframe

# ====== ä»£è™Ÿ â†” å…¬å¸åç¨±ï¼ˆå¯æ“´å……ï¼‰======
TICKER_NAME_MAP = {
    "2330.TW":"å°ç©é›»","2317.TW":"é´»æµ·","2454.TW":"è¯ç™¼ç§‘","2303.TW":"è¯é›»","2308.TW":"å°é”é›»",
    "2379.TW":"ç‘æ˜±","2382.TW":"å»£é”","2395.TW":"ç ”è¯","2408.TW":"å—äºç§‘","2412.TW":"ä¸­è¯é›»",
    "3006.TW":"æ™¶è±ªç§‘","3008.TW":"å¤§ç«‹å…‰","3711.TW":"æ—¥æœˆå…‰æŠ•æ§","2603.TW":"é•·æ¦®","2609.TW":"é™½æ˜",
    "2615.TW":"è¬æµ·","1216.TW":"çµ±ä¸€","1402.TW":"é æ±æ–°","1301.TW":"å°å¡‘","1326.TW":"å°åŒ–",
    "1101.TW":"å°æ³¥","1102.TW":"äºæ³¥","2002.TW":"ä¸­é‹¼","4904.TW":"é å‚³","3481.TW":"ç¾¤å‰µ",
    # é‡‘è
    "2880.TW":"è¯å—é‡‘","2881.TW":"å¯Œé‚¦é‡‘","2882.TW":"åœ‹æ³°é‡‘","2883.TW":"é–‹ç™¼é‡‘","2884.TW":"ç‰å±±é‡‘",
    "2885.TW":"å…ƒå¤§é‡‘","2886.TW":"å…†è±é‡‘","2887.TW":"å°æ–°é‡‘","2888.TW":"æ–°å…‰é‡‘","2889.TW":"åœ‹ç¥¨é‡‘",
    "2890.TW":"æ°¸è±é‡‘","2891.TW":"ä¸­ä¿¡é‡‘","2892.TW":"ç¬¬ä¸€é‡‘","2897.TW":"ç‹é“éŠ€è¡Œ","2898.TW":"å®‰æ³°éŠ€",
    "5871.TW":"ä¸­ç§Ÿ-KY","5876.TW":"ä¸Šæµ·å•†éŠ€"
}
FIN_TICKERS = {t for t in TICKER_NAME_MAP if t.startswith("28")}
FIN_TICKERS.update({"5871.TW","5876.TW"})

# ====== å°å·¥å…· ======
def taipei_now_str():
    return pd.Timestamp.now(tz="Asia/Taipei").strftime("%Y-%m-%d %H:%M")

def get_gspread_client():
    js = os.environ.get("GCP_SERVICE_ACCOUNT_JSON", "")
    if not js:
        raise RuntimeError("ç¼ºå°‘ GCP_SERVICE_ACCOUNT_JSON Secret")
    return gspread.service_account_from_dict(json.loads(js))

def get_sheet():
    sid = os.environ.get("SHEET_ID", "")
    if not sid:
        raise RuntimeError("ç¼ºå°‘ SHEET_ID Secret")
    print("[INFO] SHEET_ID:", sid)
    return get_gspread_client().open_by_key(sid)

def get_or_create(sh, title, rows=2000, cols=30):
    for ws in sh.worksheets():
        if ws.title == title: return ws
    return sh.add_worksheet(title=title, rows=rows, cols=cols)

def sanitize_df(df: pd.DataFrame) -> pd.DataFrame:
    """è½‰æˆ Google Sheet å‹å–„æ ¼å¼ï¼šæ—¥æœŸâ†’å­—ä¸²ã€Infâ†’NaNã€NaNâ†’Noneã€æ¬„åå­—ä¸²åŒ–"""
    out = df.copy()
    for c in out.columns:
        if np.issubdtype(out[c].dtype, np.datetime64):
            out[c] = out[c].astype(str)
    out.replace([np.inf, -np.inf], np.nan, inplace=True)
    out = out.where(pd.notnull(out), None)
    out.columns = [str(c) for c in out.columns]
    return out

def upsert_df(ws, df, stamp_text):
    ws.clear()
    ws.update("A1", [[f"è³‡æ–™æˆªè‡³ (Asia/Taipei): {stamp_text}"]])   # A1 ä¸€å¾‹ 2D list
    if df is None or df.empty:
        ws.update("A3", [["No Data"]])
        return
    clean = sanitize_df(df)
    set_with_dataframe(ws, clean, row=2, include_index=False, include_column_header=True)

# ====== æŒ‡æ¨™ ======
def add_indicators(df: pd.DataFrame) -> pd.DataFrame:
    df = df.sort_index().copy()
    # å‡ç·š
    df["SMA20"]  = df["Close"].rolling(20, min_periods=20).mean()
    df["SMA50"]  = df["Close"].rolling(50, min_periods=50).mean()
    df["SMA200"] = df["Close"].rolling(200, min_periods=200).mean()
    # æˆäº¤é‡20MA
    df["Vol20"] = df["Volume"].rolling(20, min_periods=20).mean()
    # RSI14
    delta = df["Close"].diff()
    gain = delta.clip(lower=0).rolling(14, min_periods=14).mean()
    loss = (-delta.clip(upper=0)).rolling(14, min_periods=14).mean()
    rs = gain / loss.replace(0, np.nan)
    df["RSI14"] = 100 - (100 / (1 + rs))
    # å¸ƒæ—
    mid = df["Close"].rolling(20, min_periods=20).mean()
    std = df["Close"].rolling(20, min_periods=20).std()
    df["BB_Mid"]   = mid
    df["BB_Upper"] = mid + 2 * std
    df["BB_Lower"] = mid - 2 * std
    return df

# ====== yfinance ä¸»ä¾†æº ======
def fetch_yf_history(ticker: str, period="12mo", interval="1d") -> pd.DataFrame | None:
    try:
        df = yf.download(ticker, period=period, interval=interval, auto_adjust=False, progress=False)
    except Exception:
        return None
    if df is None or df.empty:
        return None
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = [c[0] for c in df.columns]
    return df[["Open","High","Low","Close","Volume"]].copy()

# ====== TWSE å‚™æ´ï¼ˆæœˆæª”æ•´ä½µï¼‰======
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

def _twse_month_df(stock_no: str, yyyymmdd: str) -> pd.DataFrame:
    url = "https://www.twse.com.tw/exchangeReport/STOCK_DAY"
    params = {"response":"json","date":yyyymmdd,"stockNo":stock_no}
    r = requests.get(url, params=params, headers=HEADERS, timeout=12)
    r.raise_for_status()
    js = r.json()
    if js.get("stat") != "OK" or "data" not in js:
        return pd.DataFrame()
    cols = js["fields"]  # ['æ—¥æœŸ','æˆäº¤è‚¡æ•¸','æˆäº¤é‡‘é¡','é–‹ç›¤åƒ¹','æœ€é«˜åƒ¹','æœ€ä½åƒ¹','æ”¶ç›¤åƒ¹','æ¼²è·Œåƒ¹å·®','æˆäº¤ç­†æ•¸']
    df = pd.DataFrame(js["data"], columns=cols)

    def _num(x):
        try: return float(str(x).replace(",","").replace("--",""))
        except: return np.nan

    df = df.rename(columns={
        "æ—¥æœŸ":"Date","é–‹ç›¤åƒ¹":"Open","æœ€é«˜åƒ¹":"High","æœ€ä½åƒ¹":"Low","æ”¶ç›¤åƒ¹":"Close","æˆäº¤è‚¡æ•¸":"Volume"
    })
    df["Date"] = pd.to_datetime(df["Date"].str.replace("/","-"), format="%Y-%m-%d")
    for c in ["Open","High","Low","Close","Volume"]:
        df[c] = df[c].apply(_num)
    df = df[["Date","Open","High","Low","Close","Volume"]].dropna(subset=["Close"])
    return df.set_index("Date").sort_index()

def fetch_twse_history(ticker: str, months: int = 12) -> pd.DataFrame | None:
    stock_no = ticker.split(".")[0]
    today = pd.Timestamp.now(tz="Asia/Taipei")
    pieces = []
    for m in range(months):
        dt = today - pd.DateOffset(months=m)
        yyyymmdd = f"{dt.year}{dt.month:02d}01"
        try:
            dfm = _twse_month_df(stock_no, yyyymmdd)
            if not dfm.empty:
                pieces.append(dfm)
        except Exception:
            pass
        time.sleep(0.35)  # ç¯€æµ
    if not pieces:
        return None
    df = pd.concat(pieces).sort_index()
    df = df[~df.index.duplicated(keep="last")]
    return df[["Open","High","Low","Close","Volume"]]

def fetch_history_with_fallback(ticker: str) -> pd.DataFrame | None:
    df = fetch_yf_history(ticker)
    if df is not None and not df.empty:
        return df
    print(f"[INFO] yfinance ç„¡è³‡æ–™ â†’ æ”¹ç”¨ TWSEï¼š{ticker}")
    return fetch_twse_history(ticker, months=12)

# ====== äº¤æ¥æœ¬åˆ†é  ======
def update_roadmap(sh, stamp):
    ws = get_or_create(sh, "äº¤æ¥æœ¬", rows=200, cols=8)
    rows = []
    rows.append([f"äº¤æ¥æœ¬ï¼ˆè‡ªå‹•æ›´æ–°ï¼‰ï½œæœ€å¾Œæ›´æ–°ï¼š{stamp}"])
    rows.append([])
    rows.append(["å·²å®Œæˆ âœ…","èªªæ˜"])
    rows += [
        ["GitHub Actions è‡ªå‹•åŒ–","æ¯æ—¥è‡ªå‹•æŠ“ TW50 å¯«å…¥ Google Sheet"],
        ["æŠ€è¡“æŒ‡æ¨™","SMA20/50/200ã€RSI14ã€å¸ƒæ—é€šé“ã€Vol20"],
        ["åˆ†é ","TW50_fin / TW50_nonfin / Top10_nonfin / Hot20_nonfin / Top5_hot20"],
        ["é˜²å‘†æ©Ÿåˆ¶","yfinanceâ†’TWSE å‚™æ´ï¼›æŠ“ä¸åˆ°è‡ªå‹•è·³éï¼›å¯«å…¥å‰è³‡æ–™æ¶ˆæ¯’"],
    ]
    rows.append([])
    rows.append(["é€²è¡Œä¸­ ğŸ› ","èªªæ˜"])
    rows += [
        ["å‹ç‡æå‡","è¨Šè™Ÿéœ€åŒæ™‚æ»¿è¶³ï¼šæˆäº¤é‡â‰¥20æ—¥å‡é‡ ï¼‹ï¼ˆRSIâ‰¤40 æˆ– å¸ƒæ—%bâ‰¤0.10ï¼‰/ï¼ˆRSIâ‰¥60 æˆ– å¸ƒæ—%bâ‰¥0.90ï¼‰"],
        ["å…¬å¸åç¨±è£œé½Š","ä»£è™Ÿâ†”ä¸­æ–‡åç¨±ä¿åº•ä¸ç©ºç™½"],
        ["èªªæ˜åˆ†é ","å„åˆ†é å®šç¾©ï¼‹é‡‘èè‚¡è§£è®€ï¼ˆåå­˜è‚¡ã€æŠ€è¡“é¢åƒ…åƒè€ƒï¼‰"],
    ]
    rows.append([])
    rows.append(["æœªä¾† ğŸš€","èªªæ˜"])
    rows += [
        ["ç±Œç¢¼é¢","å¤–è³‡ï¼æŠ•ä¿¡ï¼è‡ªç‡Ÿå•†è²·è³£è¶…æ­é…æŠ€è¡“é¢"],
        ["åŸºæœ¬é¢","EPSã€æ®–åˆ©ç‡éæ¿¾å¼±å‹¢æ¨™çš„"],
        ["è‡ªå‹•é€šçŸ¥","LINE / Email æ¯æ—¥ Top5 è¨Šè™Ÿ"],
        ["å³æ™‚åŒ–","ç›¤ä¸­æ›´æ–°ï¼ˆéœ€åˆ¸å•† API / ä»˜è²»æ•¸æ“šï¼‰"],
        ["è‡³å°Šç‰ˆ","æŠ€è¡“ï¼‹ç±Œç¢¼ï¼‹åŸºæœ¬é¢ â†’ å¤šç©ºåˆ†æ•¸ã€Dashboard"],
    ]
    ws.clear()
    ws.update("A1", rows)

# ====== ä¸»æµç¨‹ ======
def main():
    print("== TW50 TOP5ï¼ˆyfinance + TWSE fallback + å‹ç‡ç‰ˆè¨Šè™Ÿ + äº¤æ¥æœ¬ï¼‰==")
    sh = get_sheet()
    stamp = taipei_now_str()

    # æ¸…å–®ï¼šå…ˆè®€ config.json çš„ "tickers"/"TW50"ï¼Œå¦å‰‡ç”¨å…§å»º map keys
    tickers = []
    if os.path.exists("config.json"):
        try:
            with open("config.json","r",encoding="utf-8") as f:
                cfg = json.load(f)
                tickers = cfg.get("tickers") or cfg.get("TW50") or []
        except Exception as e:
            print("[WARN] è®€å– config.json å¤±æ•—ï¼Œæ”¹ç”¨å…§å»ºæ¸…å–®", e)
    if not tickers:
        tickers = list(TICKER_NAME_MAP.keys())

    rows, failed = [], []
    for t in tickers:
        hist = fetch_history_with_fallback(t)
        if hist is None or hist.empty:
            print(f"[WARN] {t} æŸ¥ç„¡æ—¥ç·šè³‡æ–™ï¼Œå·²è·³é")
            failed.append(t)
            continue
        df = add_indicators(hist)
        last = df.tail(1).copy()
        last.insert(0, "å…¬å¸åç¨±", TICKER_NAME_MAP.get(t, ""))
        last.insert(0, "è‚¡ç¥¨ä»£è™Ÿ", t)
        last = last.reset_index().rename(columns={"index":"Date"})
        rows.append(last)

    if not rows:
        raise RuntimeError("æœ¬æ¬¡æ²’æœ‰ä»»ä½•ä»£è™ŸæˆåŠŸæŠ“åˆ°è³‡æ–™")

    df_all = pd.concat(rows, ignore_index=True)

    # é‡‘è / éé‡‘è
    is_fin = df_all["è‚¡ç¥¨ä»£è™Ÿ"].isin(FIN_TICKERS) | df_all["è‚¡ç¥¨ä»£è™Ÿ"].str.startswith("28")
    df_fin    = df_all[is_fin].copy()
    df_nonfin = df_all[~is_fin].copy()

    # å…¨é‡æ¬„ä½
    base_cols = ["è‚¡ç¥¨ä»£è™Ÿ","å…¬å¸åç¨±","Date","Open","High","Low","Close","Volume","Vol20",
                 "RSI14","SMA20","SMA50","SMA200","BB_Lower","BB_Mid","BB_Upper"]
    base_cols = [c for c in base_cols if c in df_all.columns]
    df_fin_all    = df_fin[base_cols].copy()
    df_nonfin_all = df_nonfin[base_cols].copy()

    # Top10ï¼ˆéé‡‘ï¼‰ï¼šRSIã€Volume ç”±é«˜åˆ°ä½
    top10 = df_nonfin.sort_values(["RSI14","Volume"], ascending=[False, False]).head(10).copy()

    # Hot20ï¼ˆéé‡‘ï¼‰ï¼šæˆäº¤é‡æœ€é«˜ 20
    hot20 = df_nonfin.sort_values("Volume", ascending=False).head(20).copy()

    # Top5 from Hot20ï¼ˆåŠ å…¥æ›´é«˜å‹ç‡è¨Šè™Ÿï¼‰
    top5 = hot20.sort_values(["RSI14","Volume"], ascending=[False, False]).head(5).copy()

    # å¸ƒæ—%bï¼ˆ0=è²¼è¿‘ä¸‹è»Œã€1=è²¼è¿‘ä¸Šè»Œï¼‰
    bb_range = (top5["BB_Upper"] - top5["BB_Lower"]).replace(0, np.nan)
    top5["BB_percent"] = (top5["Close"] - top5["BB_Lower"]) / bb_range

    # â€”â€” å‹ç‡æå‡ï¼šæˆäº¤é‡éæ¿¾ï¼ˆVolume å¿…é ˆ â‰¥ Vol20ï¼‰â€”â€”
    def signal_with_volume(r):
        vol_ok = pd.notna(r.get("Vol20")) and pd.notna(r.get("Volume")) and (r["Volume"] >= r["Vol20"])
        if vol_ok:
            if (pd.notna(r["BB_percent"]) and r["BB_percent"] <= 0.10) or (pd.notna(r["RSI14"]) and r["RSI14"] <= 40):
                return "è²·é€²"
            if (pd.notna(r["BB_percent"]) and r["BB_percent"] >= 0.90) or (pd.notna(r["RSI14"]) and r["RSI14"] >= 60):
                return "è³£å‡º"
        return "è§€æœ›"
    top5["è¨Šè™Ÿ"] = top5.apply(signal_with_volume, axis=1)

    # é€²/å‡ºå ´å€é–“ï¼ˆå¸ƒæ—ä¸‹~ä¸­ / ä¸­~ä¸Šï¼‰
    top5["å»ºè­°é€²å ´ä¸‹ç•Œ"] = top5["BB_Lower"]
    top5["å»ºè­°é€²å ´ä¸Šç•Œ"] = top5["BB_Mid"]
    top5["å»ºè­°å‡ºå ´ä¸‹ç•Œ"] = top5["BB_Mid"]
    top5["å»ºè­°å‡ºå ´ä¸Šç•Œ"] = top5["BB_Upper"]

    # èˆ‡é€²/å‡ºå ´ã€Œè·é›¢%ã€
    top5["è·é›¢é€²å ´%"] = np.where(
        top5["Close"] <= top5["BB_Mid"],
        (top5["Close"] - top5["BB_Lower"]) / top5["Close"] * 100,
        0.0
    )
    top5["è·é›¢å‡ºå ´%"] = np.where(
        top5["Close"] >= top5["BB_Mid"],
        (top5["BB_Upper"] - top5["Close"]) / top5["Close"] * 100,
        0.0
    )

    # Top5 æ¬„ä½è¼¸å‡ºï¼ˆä¸­æ–‡ï¼‰
    top5_cols = [
        "è‚¡ç¥¨ä»£è™Ÿ","å…¬å¸åç¨±","Date","Close","RSI14","BB_percent","è¨Šè™Ÿ",
        "å»ºè­°é€²å ´ä¸‹ç•Œ","å»ºè­°é€²å ´ä¸Šç•Œ","å»ºè­°å‡ºå ´ä¸‹ç•Œ","å»ºè­°å‡ºå ´ä¸Šç•Œ",
        "è·é›¢é€²å ´%","è·é›¢å‡ºå ´%","Volume","Vol20",
        "Open","High","Low","SMA20","SMA50","SMA200","BB_Lower","BB_Mid","BB_Upper"
    ]
    top5_out = top5[[c for c in top5_cols if c in top5.columns]].rename(
        columns={"Close":"æ”¶ç›¤åƒ¹","BB_percent":"å¸ƒæ—%b"}
    )

    # å¯«å…¥å„åˆ†é ï¼ˆå…¨é¢é˜²å‘†ï¼‰
    for title, data in [
        ("TW50_fin",    df_fin_all),
        ("TW50_nonfin", df_nonfin_all),
        ("Top10_nonfin",top10),
        ("Hot20_nonfin",hot20),
        ("Top5_hot20",  top5_out),
    ]:
        ws = get_or_create(sh, title)
        upsert_df(ws, data, stamp)
        time.sleep(0.25)

    # äº¤æ¥æœ¬
    update_roadmap(sh, stamp)

    if failed:
        print("[WARN] é€™äº›ä»£è™Ÿæ‰¾ä¸åˆ°è³‡æ–™ â†’ å·²è·³éï¼š", ", ".join(failed))
    else:
        print("[INFO] æœ¬æ¬¡æ‰€æœ‰ä»£è™Ÿçš†æˆåŠŸ")

    print("âœ… å…¨éƒ¨åˆ†é æ›´æ–°å®Œæˆ")

if __name__ == "__main__":
    main()
